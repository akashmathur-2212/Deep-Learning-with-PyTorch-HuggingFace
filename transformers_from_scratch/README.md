# Transformers from Scratch in PyTorch

This repository contains a PyTorch implementation of Transformer model entirely from scratch. All core components, including attention mechanisms, feed-forward layers, positional encoding, and the encoder-decoder architecture, have been manually implemented without relying on high-level libraries like Hugging Face.

**Features**

- Multi-Head Self-Attention

- Scaled Dot-Product Attention

- Input & Positional Encoding

- Feed-Forward Networks

- Layer Normalization

- Encoder & Decoder Blocks