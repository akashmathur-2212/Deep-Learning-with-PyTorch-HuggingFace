# LLM Evaluation Toolkit

A comprehensive guide and codebase for evaluating Large Language Models (LLMs) based appliatons using popular methods such as:

| Framework | Description |
|----------|--------------|
| [LLM-as-a-Judge](https://arxiv.org/abs/2411.15594) | Automated Evaluation |

---

## Table of Contents

- [Overview](#overview)
- [Frameworks Covered](#frameworks-covered)
- [Evaluation Methods](#evaluation-methods)
- [Exercise Notebooks](#exercise-notebooks)

---

## Overview

Evaluation of Large language models (LLMs) is often a difficult endeavour: given their broad capabilities, the tasks given to them often should be judged on requirements that would be very broad, and loosely-defined. This repository explores various frameworks, methods, and techniques to evalaute them.

---


## Exercise Notebooks

| Title	| Description	| Notebook	| Colab
|----------|-------------|--------------|--------------|
| LLM-as-a-Judge | Learn how to automate evaluations using LLM-as-a-judge | [Notebook](https://github.com/akashmathur-2212/Deep-Learning-with-PyTorch-HuggingFace/blob/main/llm-evaluation/automated_evaluation_using_llm_as_judge.ipynb) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ZR7tunTgqVenzEx_Jc2WXfg6o2XCVqt2?usp=sharing) |


---
